# CÃ¡c Thay Äá»•i Äá»ƒ Giáº£m Overfitting

## âœ… ÄÃ£ Sá»­a

### 1. **TÄƒng Dropout** (0.1 â†’ 0.3)
- ThÃªm `--dropout` argument (default: 0.3)
- Ãp dá»¥ng cho: `hidden_dropout_prob`, `attention_probs_dropout_prob`, `classifier_dropout`
- **TÃ¡c dá»¥ng:** Model khÃ´ng há»c thuá»™c lÃ²ng, pháº£i há»c quy táº¯c tháº­t

### 2. **TÄƒng Weight Decay** (0.01 â†’ 0.1)
- ThÃªm `--weight_decay` argument (default: 0.1)
- TÄƒng regularization â†’ Model khÃ´ng overfit
- **TÃ¡c dá»¥ng:** Giáº£m overfitting, model há»c quy táº¯c tá»•ng quÃ¡t hÆ¡n

### 3. **Gradient Clipping**
- ThÃªm `--max_grad_norm` argument (default: 1.0)
- Clip gradient Ä‘á»ƒ training á»•n Ä‘á»‹nh
- **TÃ¡c dá»¥ng:** TrÃ¡nh gradient explosion, training á»•n Ä‘á»‹nh hÆ¡n

### 4. **TÄƒng Label Smoothing** (0.1 â†’ 0.2)
- Default label smoothing tÄƒng tá»« 0.1 â†’ 0.2
- **TÃ¡c dá»¥ng:** Model khÃ´ng quÃ¡ tá»± tin â†’ Há»c quy táº¯c tháº­t thay vÃ¬ há»c thuá»™c

### 5. **Early Stopping Dá»±a TrÃªn Dev Loss**
- Thay vÃ¬ dÃ¹ng train loss, giá» dÃ¹ng **dev loss** Ä‘á»ƒ early stopping
- **TÃ¡c dá»¥ng:** PhÃ¡t hiá»‡n overfitting sá»›m hÆ¡n (train loss giáº£m nhÆ°ng dev loss tÄƒng)

### 6. **TÄƒng Patience** (3 â†’ 5)
- TÄƒng patience tá»« 3 â†’ 5 epochs
- **TÃ¡c dá»¥ng:** Model cÃ³ thá»i gian há»c ká»¹ hÆ¡n trÆ°á»›c khi dá»«ng

### 7. **Giáº£m Min Delta** (0.001 â†’ 0.0001)
- Giáº£m min_delta Ä‘á»ƒ nháº¡y hÆ¡n vá»›i cáº£i thiá»‡n nhá»
- **TÃ¡c dá»¥ng:** PhÃ¡t hiá»‡n cáº£i thiá»‡n tá»‘t hÆ¡n

---

## ğŸ“Š So SÃ¡nh TrÆ°á»›c/Sau

| Tham sá»‘ | TrÆ°á»›c | Sau | LÃ½ do |
|---------|-------|-----|-------|
| Dropout | 0.1 (default) | 0.3 | Giáº£m overfitting |
| Weight Decay | 0.01 | 0.1 | TÄƒng regularization |
| Label Smoothing | 0.1 | 0.2 | Model khÃ´ng quÃ¡ tá»± tin |
| Patience | 3 | 5 | Há»c ká»¹ hÆ¡n |
| Early Stopping | Train loss | **Dev loss** | PhÃ¡t hiá»‡n overfitting tá»‘t hÆ¡n |
| Gradient Clipping | KhÃ´ng cÃ³ | 1.0 | Training á»•n Ä‘á»‹nh |

---

## ğŸš€ CÃ¡ch Sá»­ Dá»¥ng

### Training vá»›i cÃ¡c tham sá»‘ má»›i (khuyáº¿n nghá»‹):

```bash
python run_bert_triple_classifier_phobert.py \
  --data_dir ./dataset \
  --bert_model vinai/phobert-base \
  --task_name kg \
  --output_dir output_moderation \
  --do_train \
  --do_eval \
  --tune_threshold \
  --loss_type label_smoothing \
  --label_smoothing 0.2 \
  --dropout 0.3 \
  --weight_decay 0.1 \
  --max_grad_norm 1.0 \
  --patience 5 \
  --learning_rate 2e-5 \
  --num_train_epochs 10 \
  --train_batch_size 32 \
  --eval_batch_size 8
```

### TÃ¹y chá»‰nh thÃªm (náº¿u váº«n overfit):

```bash
# TÄƒng dropout hÆ¡n ná»¯a
--dropout 0.4

# TÄƒng weight decay
--weight_decay 0.15

# TÄƒng label smoothing
--label_smoothing 0.3

# Giáº£m learning rate
--learning_rate 1e-5
```

---

## ğŸ¯ Ká»³ Vá»ng

Sau khi Ã¡p dá»¥ng cÃ¡c thay Ä‘á»•i nÃ y:

1. **Eval accuracy sáº½ giáº£m** (tá»« 99% â†’ khoáº£ng 85-95%)
   - âœ… ÄÃ¢y lÃ  **Tá»T** - Model khÃ´ng há»c thuá»™c ná»¯a
   - Model há»c quy táº¯c tháº­t thay vÃ¬ há»c thuá»™c pattern

2. **Gap giá»¯a train vÃ  dev loss nhá» hÆ¡n**
   - Train loss vÃ  dev loss gáº§n nhau â†’ KhÃ´ng overfit

3. **Model generalize tá»‘t hÆ¡n**
   - Dá»± Ä‘oÃ¡n Ä‘Ãºng trÃªn dá»¯ liá»‡u thá»±c táº¿
   - KhÃ´ng cÃ²n false positive nhiá»u nhÆ° trÆ°á»›c

---

## âš ï¸ LÆ°u Ã

- **Eval accuracy giáº£m lÃ  BÃŒNH THÆ¯á»œNG** - ÄÃ¢y lÃ  dáº¥u hiá»‡u model khÃ´ng há»c thuá»™c
- Quan trá»ng lÃ  **dev loss** vÃ  **test accuracy** trÃªn dá»¯ liá»‡u thá»±c táº¿
- Náº¿u váº«n overfit, tÄƒng thÃªm dropout/weight_decay
- Náº¿u underfit (accuracy quÃ¡ tháº¥p), giáº£m dropout/weight_decay

---

## ğŸ“ Checklist

- [x] ThÃªm dropout config
- [x] TÄƒng weight decay
- [x] ThÃªm gradient clipping
- [x] TÄƒng label smoothing
- [x] Early stopping dá»±a trÃªn dev loss
- [x] TÄƒng patience
- [ ] Test training vá»›i config má»›i
- [ ] So sÃ¡nh káº¿t quáº£ trÆ°á»›c/sau

